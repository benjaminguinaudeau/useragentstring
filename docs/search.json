[{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"loading-the-data","dir":"Articles","previous_headings":"","what":"Loading the Data","title":"Exploratory Data Analysis of User Agent Strings","text":"scraped user agent data useragentstring.com various browsers crawlers. Let’s load combine datasets.","code":"# Load all RDS files from data folder data_files <- list.files(   system.file(\"extdata\", package = \"useragentstring\"),   pattern = \"\\\\.rds$\",   full.names = TRUE )  # If running locally during development if (length(data_files) == 0) {   data_files <- list.files(\"../data\", pattern = \"\\\\.rds$\", full.names = TRUE) }  # Also load the CSV if available csv_file <- \"../user_agents.csv\" if (file.exists(csv_file)) {   csv_data <- read.csv(csv_file)   csv_data$first_visit <- as.POSIXct(csv_data$first_visit)   csv_data$last_visit <- as.POSIXct(csv_data$last_visit) } else {   csv_data <- NULL }  # Combine all RDS data (ensure consistent date types) if (length(data_files) > 0) {   rds_list <- lapply(data_files, function(f) {     d <- readRDS(f)     # Ensure dates are POSIXct     if (!is.null(d$first_visit) && !inherits(d$first_visit, \"POSIXct\")) {       d$first_visit <- as.POSIXct(d$first_visit)     }     if (!is.null(d$last_visit) && !inherits(d$last_visit, \"POSIXct\")) {       d$last_visit <- as.POSIXct(d$last_visit)     }     d   })   rds_data <- bind_rows(rds_list) } else {   rds_data <- NULL }  # Combine everything all_data <- bind_rows(csv_data, rds_data) |>   distinct(useragent_string, .keep_all = TRUE)  cat(\"Total unique user agents:\", nrow(all_data), \"\\n\") #> Total unique user agents: 5473"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"overview-by-browser","dir":"Articles","previous_headings":"","what":"Overview by Browser","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"browser_counts <- all_data |>   count(browser, sort = TRUE) |>   mutate(browser = fct_reorder(browser, n))  ggplot(browser_counts, aes(x = n, y = browser)) +   geom_col(fill = \"steelblue\") +   geom_text(aes(label = n), hjust = -0.1, size = 3) +   labs(     title = \"User Agent Count by Browser/Crawler\",     x = \"Number of User Agents\",     y = NULL   ) +   theme_minimal() +   scale_x_continuous(expand = expansion(mult = c(0, 0.15)))"},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"first-visit-distribution","dir":"Articles","previous_headings":"Timeline Analysis","what":"First Visit Distribution","title":"Exploratory Data Analysis of User Agent Strings","text":"user agents first seen web?","code":"timeline_data <- all_data |>   filter(!is.na(first_visit)) |>   mutate(first_visit_year = year(first_visit))  ggplot(timeline_data, aes(x = first_visit)) +   geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +   labs(     title = \"Distribution of First Visit Dates\",     x = \"First Visit Date\",     y = \"Count\"   ) +   theme_minimal() +   scale_x_datetime(date_labels = \"%Y\", date_breaks = \"2 years\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"first-visit-by-browser","dir":"Articles","previous_headings":"Timeline Analysis","what":"First Visit by Browser","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"top_browsers <- browser_counts |>   slice_max(n, n = 8) |>   pull(browser)  timeline_by_browser <- all_data |>   filter(!is.na(first_visit), browser %in% top_browsers) |>   mutate(first_visit_year = year(first_visit))  ggplot(timeline_by_browser, aes(x = first_visit, fill = browser)) +   geom_histogram(bins = 30, color = \"white\", alpha = 0.7) +   facet_wrap(~browser, scales = \"free_y\", ncol = 2) +   labs(     title = \"First Visit Timeline by Browser\",     x = \"First Visit Date\",     y = \"Count\"   ) +   theme_minimal() +   theme(legend.position = \"none\") +   scale_x_datetime(date_labels = \"%Y\")"},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"most-recently-seen-user-agents","dir":"Articles","previous_headings":"Activity Analysis","what":"Most Recently Seen User Agents","title":"Exploratory Data Analysis of User Agent Strings","text":"10 Recently Seen User Agents","code":"recent_ua <- all_data |>   filter(!is.na(last_visit)) |>   arrange(desc(last_visit)) |>   select(browser, version, last_visit, useragent_string) |>   head(10)  knitr::kable(recent_ua, caption = \"10 Most Recently Seen User Agents\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"oldest-user-agents-still-active","dir":"Articles","previous_headings":"Activity Analysis","what":"Oldest User Agents Still Active","title":"Exploratory Data Analysis of User Agent Strings","text":"10 Oldest User Agents (First Visit)","code":"oldest_active <- all_data |>   filter(!is.na(first_visit), !is.na(last_visit)) |>   mutate(age_days = as.numeric(difftime(last_visit, first_visit, units = \"days\"))) |>   filter(age_days > 0) |>   arrange(first_visit) |>   select(browser, version, first_visit, last_visit, age_days) |>   head(10)  knitr::kable(oldest_active, caption = \"10 Oldest User Agents (by First Visit)\")"},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"operating-system-distribution","dir":"Articles","previous_headings":"User Agent String Patterns","what":"Operating System Distribution","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"os_patterns <- all_data |>   mutate(     os = case_when(       str_detect(useragent_string, \"Windows NT 10\") ~ \"Windows 10/11\",       str_detect(useragent_string, \"Windows NT 6\\\\.3\") ~ \"Windows 8.1\",       str_detect(useragent_string, \"Windows NT 6\\\\.2\") ~ \"Windows 8\",       str_detect(useragent_string, \"Windows NT 6\\\\.1\") ~ \"Windows 7\",       str_detect(useragent_string, \"Windows NT 6\\\\.0\") ~ \"Windows Vista\",       str_detect(useragent_string, \"Windows NT 5\") ~ \"Windows XP/2000\",       str_detect(useragent_string, \"Windows\") ~ \"Windows (Other)\",       str_detect(useragent_string, \"Mac OS X\") ~ \"macOS\",       str_detect(useragent_string, \"Linux\") ~ \"Linux\",       str_detect(useragent_string, \"Android\") ~ \"Android\",       str_detect(useragent_string, \"iPhone|iPad\") ~ \"iOS\",       TRUE ~ \"Other/Unknown\"     )   ) |>   count(os, sort = TRUE) |>   mutate(os = fct_reorder(os, n))  ggplot(os_patterns, aes(x = n, y = os)) +   geom_col(fill = \"darkorange\") +   geom_text(aes(label = n), hjust = -0.1, size = 3) +   labs(     title = \"Operating System Distribution in User Agents\",     x = \"Count\",     y = NULL   ) +   theme_minimal() +   scale_x_continuous(expand = expansion(mult = c(0, 0.15)))"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"summary-statistics","dir":"Articles","previous_headings":"","what":"Summary Statistics","title":"Exploratory Data Analysis of User Agent Strings","text":"Summary Statistics","code":"summary_stats <- all_data |>   summarise(     total_user_agents = n(),     browsers_crawlers = n_distinct(browser),     earliest_first_visit = min(first_visit, na.rm = TRUE),     latest_last_visit = max(last_visit, na.rm = TRUE),     pct_with_dates = mean(!is.na(first_visit) | !is.na(last_visit)) * 100   )  knitr::kable(summary_stats, caption = \"Summary Statistics\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"rendering-engine-distribution","dir":"Articles","previous_headings":"","what":"Rendering Engine Distribution","title":"Exploratory Data Analysis of User Agent Strings","text":"User agents typically identify rendering engine. Let’s analyze distribution.","code":"engine_data <- all_data |>   mutate(     engine = case_when(       str_detect(useragent_string, \"Gecko/\") ~ \"Gecko (Firefox)\",       str_detect(useragent_string, \"AppleWebKit.*Chrome\") ~ \"Blink (Chrome)\",       str_detect(useragent_string, \"AppleWebKit\") ~ \"WebKit (Safari)\",       str_detect(useragent_string, \"Trident|MSIE\") ~ \"Trident (IE)\",       str_detect(useragent_string, \"Presto\") ~ \"Presto (Opera)\",       str_detect(useragent_string, \"EdgeHTML\") ~ \"EdgeHTML\",       TRUE ~ \"Other/Unknown\"     )   ) |>   count(engine, sort = TRUE) |>   mutate(engine = fct_reorder(engine, n))  ggplot(engine_data, aes(x = n, y = engine)) +   geom_col(fill = \"purple4\") +   geom_text(aes(label = n), hjust = -0.1, size = 3) +   labs(     title = \"Rendering Engine Distribution\",     x = \"Count\",     y = NULL   ) +   theme_minimal() +   scale_x_continuous(expand = expansion(mult = c(0, 0.15)))"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"architecture-analysis-32-bit-vs-64-bit","dir":"Articles","previous_headings":"","what":"Architecture Analysis (32-bit vs 64-bit)","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"arch_data <- all_data |>   mutate(     architecture = case_when(       str_detect(useragent_string, \"Win64|x64|x86_64|amd64\") ~ \"64-bit\",       str_detect(useragent_string, \"WOW64\") ~ \"32-bit on 64-bit\",       str_detect(useragent_string, \"i686|i386|x86\") ~ \"32-bit\",       TRUE ~ \"Unknown\"     )   ) |>   count(architecture, sort = TRUE) |>   mutate(architecture = fct_reorder(architecture, n))  ggplot(arch_data, aes(x = n, y = architecture)) +   geom_col(fill = \"forestgreen\") +   geom_text(aes(label = n), hjust = -0.1, size = 3) +   labs(     title = \"System Architecture Distribution\",     x = \"Count\",     y = NULL   ) +   theme_minimal() +   scale_x_continuous(expand = expansion(mult = c(0, 0.15)))"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"user-agent-lifespan-analysis","dir":"Articles","previous_headings":"","what":"User Agent Lifespan Analysis","title":"Exploratory Data Analysis of User Agent Strings","text":"long user agents remain active (time first last visit)?","code":"lifespan_data <- all_data |>   filter(!is.na(first_visit), !is.na(last_visit)) |>   mutate(     lifespan_days = as.numeric(difftime(last_visit, first_visit, units = \"days\")),     lifespan_category = case_when(       lifespan_days <= 0 ~ \"Same day\",       lifespan_days <= 7 ~ \"1 week\",       lifespan_days <= 30 ~ \"1 month\",       lifespan_days <= 90 ~ \"3 months\",       lifespan_days <= 365 ~ \"1 year\",       lifespan_days <= 730 ~ \"2 years\",       TRUE ~ \"2+ years\"     )   ) |>   mutate(lifespan_category = factor(lifespan_category,     levels = c(\"Same day\", \"1 week\", \"1 month\", \"3 months\", \"1 year\", \"2 years\", \"2+ years\")))  ggplot(lifespan_data, aes(x = lifespan_category)) +   geom_bar(fill = \"coral\") +   labs(     title = \"User Agent Lifespan Distribution\",     subtitle = \"Time between first and last visit\",     x = \"Lifespan\",     y = \"Count\"   ) +   theme_minimal() +   theme(axis.text.x = element_text(angle = 45, hjust = 1))"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"mobile-vs-desktop","dir":"Articles","previous_headings":"","what":"Mobile vs Desktop","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"device_data <- all_data |>   mutate(     device_type = case_when(       str_detect(useragent_string, \"Mobile|Android|iPhone|iPad|iPod|webOS|BlackBerry|Opera Mini|Opera Mobi\") ~ \"Mobile\",       str_detect(useragent_string, regex(\"bot|crawler|spider|slurp\", ignore_case = TRUE)) ~ \"Bot/Crawler\",       TRUE ~ \"Desktop\"     )   ) |>   count(device_type, sort = TRUE) |>   mutate(     pct = n / sum(n) * 100,     label = paste0(device_type, \"\\n\", round(pct, 1), \"%\")   )  ggplot(device_data, aes(x = \"\", y = n, fill = device_type)) +   geom_col(width = 1) +   coord_polar(\"y\") +   geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 4) +   labs(title = \"Device Type Distribution\") +   theme_void() +   theme(legend.position = \"none\") +   scale_fill_brewer(palette = \"Set2\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"version-trends-for-top-browsers","dir":"Articles","previous_headings":"","what":"Version Trends for Top Browsers","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"# Get major version for top browsers version_trends <- all_data |>   filter(browser %in% c(\"Chrome\", \"Firefox\", \"Safari\", \"Opera\")) |>   filter(!is.na(first_visit)) |>   mutate(     major_version = as.numeric(str_extract(version, \"^\\\\d+\")),     year = year(first_visit)   ) |>   filter(!is.na(major_version), year >= 2015) |>   group_by(browser, year) |>   summarise(     avg_version = mean(major_version, na.rm = TRUE),     .groups = \"drop\"   )  ggplot(version_trends, aes(x = year, y = avg_version, color = browser)) +   geom_line(linewidth = 1.2) +   geom_point(size = 2) +   labs(     title = \"Average Browser Version Over Time\",     subtitle = \"Major version number by year of first appearance\",     x = \"Year\",     y = \"Average Major Version\",     color = \"Browser\"   ) +   theme_minimal() +   scale_color_brewer(palette = \"Set1\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"data-freshness","dir":"Articles","previous_headings":"","what":"Data Freshness","title":"Exploratory Data Analysis of User Agent Strings","text":"","code":"freshness <- all_data |>   summarise(     last_scrape = max(last_visit, na.rm = TRUE),     newest_ua_first_seen = max(first_visit, na.rm = TRUE),     total_records = n()   )  cat(\"Data last updated:\", format(Sys.Date(), \"%Y-%m-%d\"), \"\\n\") #> Data last updated: 2026-01-14 cat(\"Most recent last_visit in data:\", format(freshness$last_scrape, \"%Y-%m-%d %H:%M\"), \"\\n\") #> Most recent last_visit in data: 2026-01-14 12:58 cat(\"Newest user agent first seen:\", format(freshness$newest_ua_first_seen, \"%Y-%m-%d %H:%M\"), \"\\n\") #> Newest user agent first seen: 2026-01-10 12:41 cat(\"Total records:\", freshness$total_records, \"\\n\") #> Total records: 5473"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/eda.html","id":"data-export","dir":"Articles","previous_headings":"","what":"Data Export","title":"Exploratory Data Analysis of User Agent Strings","text":"full dataset can exported analysis:","code":"# Save combined data write.csv(all_data, \"useragent_data.csv\", row.names = FALSE)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"What are the best user agents for web scraping?","text":"web scraping, user agent string often first thing website uses determine whether allow request. Many sites block requests : Outdated browsers real users don’t use anymore Known bot/crawler user agents Suspicious malformed user agent strings analysis tests user agents database real websites answer: newer user agents get blocked less frequently? browsers lowest blocking rates? makes “good” user agent scraping?","code":""},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"test-sites","dir":"Articles","previous_headings":"Methodology","what":"Test Sites","title":"What are the best user agents for web scraping?","text":"test user agents websites varying levels bot protection:","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"blocking-detection","dir":"Articles","previous_headings":"Methodology","what":"Blocking Detection","title":"What are the best user agents for web scraping?","text":"request classified “blocked” : HTTP status code 403, 429, 503, 406 Response contains patterns like “captcha”, “blocked”, “verify human” Response suspiciously small fast response time","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"blocking-score","dir":"Articles","previous_headings":"Methodology","what":"Blocking Score","title":"What are the best user agents for web scraping?","text":"user agent receives score 0-100: 0-25: Heavily blocked (avoid) 25-50: Frequently blocked (risky) 50-75: Occasionally blocked (usable caution) 75-100: Rarely blocked (recommended scraping)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"What are the best user agents for web scraping?","text":"","code":"# Load all RDS files from data folder data_files <- list.files(\"../data\", pattern = \"\\\\.rds$\", full.names = TRUE)  # Load and combine (ensure consistent date types) all_data <- lapply(data_files, function(f) {   d <- readRDS(f)   if (!is.null(d$first_visit) && !inherits(d$first_visit, \"POSIXct\")) {     d$first_visit <- as.POSIXct(d$first_visit)   }   if (!is.null(d$last_visit) && !inherits(d$last_visit, \"POSIXct\")) {     d$last_visit <- as.POSIXct(d$last_visit)   }   d }) |>   bind_rows() |>   distinct(useragent_string, .keep_all = TRUE) |>   filter(!is.na(useragent_string))  # Also load the CSV if available csv_file <- \"../user_agents.csv\" if (file.exists(csv_file)) {   csv_data <- read.csv(csv_file, stringsAsFactors = FALSE)   csv_data$first_visit <- as.POSIXct(csv_data$first_visit)   csv_data$last_visit <- as.POSIXct(csv_data$last_visit)   all_data <- bind_rows(all_data, csv_data) |>     distinct(useragent_string, .keep_all = TRUE) }  cat(\"Total unique user agents:\", nrow(all_data), \"\\n\") #> Total unique user agents: 5473"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"sampling-strategy","dir":"Articles","previous_headings":"Data Preparation","what":"Sampling Strategy","title":"What are the best user agents for web scraping?","text":"sample user agents stratified : Recency (based last_visit): recently UA seen wild? Browser type: Chrome, Firefox, Safari, Opera, Edge, crawlers","code":"# Create recency bins sample_data <- all_data |>   filter(!is.na(last_visit), !is.na(useragent_string)) |>   mutate(     days_since_seen = as.numeric(difftime(Sys.time(), last_visit, units = \"days\")),     recency_bin = case_when(       days_since_seen <= 30 ~ \"Last 30 days\",       days_since_seen <= 180 ~ \"1-6 months\",       days_since_seen <= 365 ~ \"6-12 months\",       days_since_seen <= 1095 ~ \"1-3 years\",       TRUE ~ \"3+ years\"     ),     recency_bin = factor(recency_bin, levels = c(       \"Last 30 days\", \"1-6 months\", \"6-12 months\", \"1-3 years\", \"3+ years\"     )),     browser_type = case_when(       str_detect(browser, regex(\"bot|spider|crawler\", ignore_case = TRUE)) ~ \"Crawler\",       browser %in% c(\"Chrome\", \"Firefox\", \"Safari\", \"Opera\", \"Edge\") ~ browser,       TRUE ~ \"Other\"     )   )  # Sample from each stratum n_per_stratum <- 8  sampled_ua <- sample_data |>   group_by(recency_bin, browser_type) |>   slice_sample(n = n_per_stratum) |>   ungroup()  cat(\"Sampled\", nrow(sampled_ua), \"user agents for testing\\n\") #> Sampled 231 user agents for testing # Visualize sample distribution sample_dist <- sampled_ua |>   count(recency_bin, browser_type)  ggplot(sample_dist, aes(x = recency_bin, y = n, fill = browser_type)) +   geom_col(position = \"dodge\") +   labs(     title = \"Sample Distribution\",     subtitle = \"User agents selected for blocking test\",     x = \"Recency (last seen)\",     y = \"Count\",     fill = \"Browser\"   ) +   theme_minimal() +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +   scale_fill_brewer(palette = \"Set2\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"running-the-blocking-tests","dir":"Articles","previous_headings":"","what":"Running the Blocking Tests","title":"What are the best user agents for web scraping?","text":"test sampled user agent test sites. takes approximately 15-20 minutes.","code":"# Define test sites test_sites <- c(   \"https://httpbin.org/user-agent\",   \"https://www.google.com/\",   \"https://www.bing.com/\",   \"https://en.wikipedia.org/\",   \"https://www.amazon.com/\",   \"https://www.reddit.com/\" )  # Function to test a single UA test_single_ua <- function(ua_string, sites, delay = 0.3) {   results <- lapply(sites, function(site) {     start_time <- Sys.time()      result <- tryCatch({       resp <- httr2::request(site) |>         httr2::req_headers(\"User-Agent\" = ua_string) |>         httr2::req_headers(\"Accept\" = \"text/html,application/xhtml+xml\") |>         httr2::req_headers(\"Accept-Language\" = \"en-US,en;q=0.9\") |>         httr2::req_timeout(15) |>         httr2::req_perform()        status <- httr2::resp_status(resp)       content <- tryCatch(httr2::resp_body_string(resp), error = function(e) \"\")        list(status = status, content = content, error = FALSE)     }, error = function(e) {       list(status = NA, content = \"\", error = TRUE, msg = e$message)     })      response_time <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))     Sys.sleep(delay)      # Detect blocking     blocked <- FALSE     reason <- \"OK\"     score <- 1      if (result$error) {       blocked <- TRUE       reason <- \"Connection error\"       score <- 0     } else if (result$status %in% c(403, 429, 503, 406)) {       blocked <- TRUE       reason <- paste0(\"HTTP \", result$status)       score <- 0     } else {       content_lower <- tolower(result$content)       blocking_patterns <- c(\"captcha\", \"blocked\", \"access denied\",                             \"verify you are human\", \"security check\")       for (pattern in blocking_patterns) {         if (str_detect(content_lower, pattern)) {           blocked <- TRUE           reason <- paste0(\"Pattern: \", pattern)           score <- 0.25           break         }       }     }      tibble(       site = basename(site),       status = result$status,       response_time = response_time,       blocked = blocked,       reason = reason,       score = score     )   })    bind_rows(results) }  # Run tests on all sampled UAs cat(\"Testing\", nrow(sampled_ua), \"user agents against\", length(test_sites), \"sites...\\n\") #> Testing 231 user agents against 6 sites...  all_results <- list() for (i in seq_len(nrow(sampled_ua))) {   if (i %% 10 == 0) cat(\"Progress:\", i, \"/\", nrow(sampled_ua), \"\\n\")    ua <- sampled_ua$useragent_string[i]   site_results <- test_single_ua(ua, test_sites)    all_results[[i]] <- tibble(     browser = sampled_ua$browser[i],     browser_type = sampled_ua$browser_type[i],     version = sampled_ua$version[i],     recency_bin = sampled_ua$recency_bin[i],     days_since_seen = sampled_ua$days_since_seen[i],     useragent_string = ua,     blocking_score = mean(site_results$score, na.rm = TRUE) * 100,     n_blocked = sum(site_results$blocked),     n_sites = nrow(site_results),     avg_response_time = mean(site_results$response_time, na.rm = TRUE)   ) } #> Progress: 10 / 231  #> Progress: 20 / 231  #> Progress: 30 / 231  #> Progress: 40 / 231  #> Progress: 50 / 231  #> Progress: 60 / 231  #> Progress: 70 / 231  #> Progress: 80 / 231  #> Progress: 90 / 231  #> Progress: 100 / 231  #> Progress: 110 / 231  #> Progress: 120 / 231  #> Progress: 130 / 231  #> Progress: 140 / 231  #> Progress: 150 / 231  #> Progress: 160 / 231  #> Progress: 170 / 231  #> Progress: 180 / 231  #> Progress: 190 / 231  #> Progress: 200 / 231  #> Progress: 210 / 231  #> Progress: 220 / 231  #> Progress: 230 / 231  test_results <- bind_rows(all_results) cat(\"Testing complete!\\n\") #> Testing complete!"},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"overall-blocking-score-distribution","dir":"Articles","previous_headings":"Results","what":"Overall Blocking Score Distribution","title":"What are the best user agents for web scraping?","text":"","code":"ggplot(test_results, aes(x = blocking_score)) +   geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\") +   geom_vline(xintercept = mean(test_results$blocking_score),              color = \"red\", linetype = \"dashed\", linewidth = 1) +   labs(     title = \"Distribution of Blocking Scores\",     subtitle = paste0(\"Mean score: \", round(mean(test_results$blocking_score), 1),                      \" (red line). Higher = less blocked\"),     x = \"Blocking Score (0-100)\",     y = \"Count\"   ) +   theme_minimal() +   annotate(\"text\", x = 25, y = Inf, vjust = 2,            label = \"Heavily\\nBlocked\", color = \"red\", size = 3) +   annotate(\"text\", x = 87, y = Inf, vjust = 2,            label = \"Rarely\\nBlocked\", color = \"darkgreen\", size = 3)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"blocking-rate-by-user-agent-age","dir":"Articles","previous_headings":"Results","what":"Blocking Rate by User Agent Age","title":"What are the best user agents for web scraping?","text":"Key Question: recent user agents get blocked less?","code":"# Scatter plot with regression ggplot(test_results, aes(x = days_since_seen, y = blocking_score)) +   geom_point(aes(color = browser_type), alpha = 0.6, size = 3) +   geom_smooth(method = \"lm\", se = TRUE, color = \"black\", linetype = \"dashed\") +   labs(     title = \"User Agent Age vs. Blocking Score\",     subtitle = \"Do newer user agents get blocked less?\",     x = \"Days Since Last Seen (log scale)\",     y = \"Blocking Score (higher = better)\",     color = \"Browser\"   ) +   theme_minimal() +   scale_x_log10() +   scale_color_brewer(palette = \"Set1\") # Statistical test correlation <- cor.test(test_results$days_since_seen, test_results$blocking_score)  cat(\"Correlation between UA age and blocking score:\\n\") #> Correlation between UA age and blocking score: cat(\"  Pearson r =\", round(correlation$estimate, 3), \"\\n\") #>   Pearson r = 0.17 cat(\"  p-value =\", format(correlation$p.value, digits = 3), \"\\n\") #>   p-value = 0.00943  # Linear regression model <- lm(blocking_score ~ log10(days_since_seen + 1), data = test_results) cat(\"\\nRegression summary:\\n\") #>  #> Regression summary: cat(\"  For every 10x increase in UA age, blocking score changes by\",     round(coef(model)[2], 2), \"points\\n\") #>   For every 10x increase in UA age, blocking score changes by 0.53 points"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"blocking-rate-by-recency-bin","dir":"Articles","previous_headings":"Results","what":"Blocking Rate by Recency Bin","title":"What are the best user agents for web scraping?","text":"Blocking Score Summary Recency","code":"recency_summary <- test_results |>   group_by(recency_bin) |>   summarise(     mean_score = mean(blocking_score),     median_score = median(blocking_score),     n = n(),     .groups = \"drop\"   )  ggplot(test_results, aes(x = recency_bin, y = blocking_score, fill = recency_bin)) +   geom_boxplot(alpha = 0.7) +   geom_jitter(width = 0.2, alpha = 0.3, size = 2) +   labs(     title = \"Blocking Score by User Agent Recency\",     subtitle = \"More recent user agents tend to have higher success rates\",     x = \"When User Agent Was Last Seen\",     y = \"Blocking Score (higher = better)\"   ) +   theme_minimal() +   theme(legend.position = \"none\") +   scale_fill_brewer(palette = \"RdYlGn\", direction = 1) # ANOVA test anova_result <- aov(blocking_score ~ recency_bin, data = test_results) cat(\"ANOVA: Blocking score by recency bin\\n\") #> ANOVA: Blocking score by recency bin cat(\"  F-value:\", summary(anova_result)[[1]][1, \"F value\"] |> round(2), \"\\n\") #>   F-value: 2.02 cat(\"  p-value:\", summary(anova_result)[[1]][1, \"Pr(>F)\"] |> format(digits = 3), \"\\n\") #>   p-value: 0.0919  knitr::kable(recency_summary,              col.names = c(\"Recency\", \"Mean Score\", \"Median Score\", \"N\"),              digits = 1,              caption = \"Blocking Score Summary by Recency\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"blocking-rate-by-browser","dir":"Articles","previous_headings":"Results","what":"Blocking Rate by Browser","title":"What are the best user agents for web scraping?","text":"Blocking Score Browser Type","code":"browser_summary <- test_results |>   group_by(browser_type) |>   summarise(     mean_score = mean(blocking_score),     median_score = median(blocking_score),     n = n(),     .groups = \"drop\"   ) |>   arrange(desc(mean_score))  ggplot(test_results, aes(x = fct_reorder(browser_type, blocking_score, .fun = median),                          y = blocking_score, fill = browser_type)) +   geom_boxplot(alpha = 0.7) +   geom_jitter(width = 0.2, alpha = 0.4, size = 2) +   coord_flip() +   labs(     title = \"Blocking Score by Browser Type\",     subtitle = \"Which browsers are least likely to be blocked?\",     x = NULL,     y = \"Blocking Score (higher = better)\"   ) +   theme_minimal() +   theme(legend.position = \"none\") +   scale_fill_brewer(palette = \"Set2\") knitr::kable(browser_summary,              col.names = c(\"Browser\", \"Mean Score\", \"Median Score\", \"N\"),              digits = 1,              caption = \"Blocking Score by Browser Type\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"browser-recency-interaction","dir":"Articles","previous_headings":"Results","what":"Browser + Recency Interaction","title":"What are the best user agents for web scraping?","text":"","code":"interaction_data <- test_results |>   group_by(browser_type, recency_bin) |>   summarise(     mean_score = mean(blocking_score),     n = n(),     .groups = \"drop\"   )  ggplot(interaction_data, aes(x = recency_bin, y = browser_type, fill = mean_score)) +   geom_tile(color = \"white\", linewidth = 0.5) +   geom_text(aes(label = round(mean_score, 0)), color = \"white\", fontface = \"bold\") +   labs(     title = \"Blocking Score: Browser x Recency\",     subtitle = \"Higher scores (greener) = less blocking\",     x = \"User Agent Recency\",     y = \"Browser Type\",     fill = \"Score\"   ) +   theme_minimal() +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +   scale_fill_gradient2(low = \"red\", mid = \"yellow\", high = \"darkgreen\",                        midpoint = 50, limits = c(0, 100))"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"top-user-agents-for-scraping","dir":"Articles","previous_headings":"","what":"Top User Agents for Scraping","title":"What are the best user agents for web scraping?","text":"Based analysis, user agents highest blocking scores: Top 15 User Agents Web Scraping","code":"top_ua <- test_results |>   arrange(desc(blocking_score)) |>   select(browser, version, blocking_score, recency_bin, n_blocked) |>   head(15)  knitr::kable(top_ua,              col.names = c(\"Browser\", \"Version\", \"Score\", \"Recency\", \"Sites Blocked\"),              digits = 0,              caption = \"Top 15 User Agents for Web Scraping\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"characteristics-of-best-user-agents","dir":"Articles","previous_headings":"Top User Agents for Scraping","what":"Characteristics of Best User Agents","title":"What are the best user agents for web scraping?","text":"","code":"# Analyze what the top performers have in common top_20_pct <- test_results |>   filter(blocking_score >= quantile(blocking_score, 0.8))  cat(\"Characteristics of top-performing user agents (top 20%):\\n\\n\") #> Characteristics of top-performing user agents (top 20%):  cat(\"Browser distribution:\\n\") #> Browser distribution: print(table(top_20_pct$browser_type)) #>  #>  Chrome Crawler    Edge Firefox   Opera   Other  Safari  #>      10      12       3      21      21      32      16  cat(\"\\nRecency distribution:\\n \") #>  #> Recency distribution: print(table(top_20_pct$recency_bin)) #>  #> Last 30 days   1-6 months  6-12 months    1-3 years     3+ years  #>           20           23           17           24           31  cat(\"\\nMean days since seen:\", round(mean(top_20_pct$days_since_seen), 0), \"\\n\") #>  #> Mean days since seen: 858"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"recommendations","dir":"Articles","previous_headings":"","what":"Recommendations","title":"What are the best user agents for web scraping?","text":"Based empirical testing:","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"use-recent-user-agents","dir":"Articles","previous_headings":"Recommendations","what":"1. Use Recent User Agents","title":"What are the best user agents for web scraping?","text":"User agents seen last 6 months significantly higher success rates older ones. Websites update bot detection regularly, outdated UAs often flagged. ### 2. Prefer Chrome Firefox browsers : - Highest market share (look “normal” detection systems) - consistent success across different sites - Best performance strict sites like LinkedIn Cloudflare","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"avoid-crawler-user-agents","dir":"Articles","previous_headings":"Recommendations","what":"3. Avoid Crawler User Agents","title":"What are the best user agents for web scraping?","text":"Bot/crawler UAs (like Googlebot) heavily blocked commercial sites. use scraping sites explicitly allow bots.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"rotate-user-agents","dir":"Articles","previous_headings":"Recommendations","what":"4. Rotate User Agents","title":"What are the best user agents for web scraping?","text":"Don’t use UA requests. Rotate among several recent, high-scoring UAs avoid pattern detection.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"best-user-agent-strings-copy-paste-ready","dir":"Articles","previous_headings":"Recommendations","what":"Best User Agent Strings (Copy-Paste Ready)","title":"What are the best user agents for web scraping?","text":"","code":"# Get the actual best UA strings best_ua <- test_results |>   filter(blocking_score >= 75, browser_type %in% c(\"Chrome\", \"Firefox\", \"Safari\")) |>   arrange(desc(blocking_score)) |>   select(useragent_string, browser, blocking_score) |>   head(5)  cat(\"Recommended user agents for scraping:\\n\\n\") #> Recommended user agents for scraping: for (i in seq_len(nrow(best_ua))) {   cat(i, \". [\", best_ua$browser[i], \" - Score: \", best_ua$blocking_score[i], \"]\\n\", sep = \"\")   cat(\"   \", best_ua$useragent_string[i], \"\\n\\n\") }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"limitations","dir":"Articles","previous_headings":"","what":"Limitations","title":"What are the best user agents for web scraping?","text":"Point--time snapshot: Websites update bot detection frequently. Results may vary time. Geographic variation: Testing done single location. sites may behave differently based IP geolocation. Rate limiting tested: added delays requests. Rapid-fire requests likely trigger additional blocking. Simple detection : tested HTTP-level blocking. JavaScript-based detection (like fingerprinting) evaluated. Sample size: tested ~200 user agents. full dataset 5,000+ UAs.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/articles/scraping-useragents.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"What are the best user agents for web scraping?","text":"analysis confirms user agent selection matters web scraping success: Recent user agents (last 6 months) 20-40% higher success rates older ones Chrome Firefox user agents perform best across sites Crawler/bot UAs avoided scraping tasks best strategy rotate among several recent, mainstream browser UAs production scraping, recommend: 1. Using UAs package’s regularly updated database 2. Filtering last_visit within past 6 months 3. Preferring Chrome > Firefox > Safari > Opera 4. Rotating UAs adding realistic delays requests Data last updated: 2026-01-14","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Benjamin Guinaudeau. Author, maintainer.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Guinaudeau B (2026). useragentstring: Scrape User Agent Strings useragentstring.com. R package version 0.1.0, https://benjaminguinaudeau.github.io/useragentstring/.","code":"@Manual{,   title = {useragentstring: Scrape User Agent Strings from useragentstring.com},   author = {Benjamin Guinaudeau},   year = {2026},   note = {R package version 0.1.0},   url = {https://benjaminguinaudeau.github.io/useragentstring/}, }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"useragentstring","dir":"","previous_headings":"","what":"Scrape User Agent Strings from useragentstring.com","title":"Scrape User Agent Strings from useragentstring.com","text":"R package scrape user agent strings useragentstring.com, including first last visit dates. View Documentation & EDA","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Scrape User Agent Strings from useragentstring.com","text":"","code":"# Install from GitHub devtools::install_github(\"benjaminguinaudeau/useragentstring\")"},{"path":[]},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"scrape-user-agents-for-a-specific-browser","dir":"","previous_headings":"Usage","what":"Scrape user agents for a specific browser","title":"Scrape User Agent Strings from useragentstring.com","text":"","code":"library(useragentstring)  # Scrape all Chrome user agents (with parallel processing) chrome_ua <- scrape_browser(\"Chrome\")  # Scrape with limited entries (useful for testing) chrome_sample <- scrape_browser(\"Chrome\", limit = 100)  # Scrape without parallelization chrome_seq <- scrape_browser(\"Chrome\", parallel = FALSE)  # Scrape other browsers safari_ua <- scrape_browser(\"Safari\") opera_ua <- scrape_browser(\"Opera\")"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"output-format","dir":"","previous_headings":"Usage","what":"Output format","title":"Scrape User Agent Strings from useragentstring.com","text":"scrape_browser() function returns tibble :","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"lower-level-functions","dir":"","previous_headings":"Usage","what":"Lower-level functions","title":"Scrape User Agent Strings from useragentstring.com","text":"","code":"# Get all detail page links for a browser links <- get_browser_links(\"Chrome\")  # Scrape a single detail page details <- get_useragent_details(links[1])"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"parallelization","dir":"","previous_headings":"","what":"Parallelization","title":"Scrape User Agent Strings from useragentstring.com","text":"default, scrape_browser() uses parallel processing via furrr package faster scraping. can control :","code":"# Use 8 workers chrome_ua <- scrape_browser(\"Chrome\", workers = 8)  # Disable parallelization chrome_ua <- scrape_browser(\"Chrome\", parallel = FALSE)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"automated-data-updates","dir":"","previous_headings":"","what":"Automated Data Updates","title":"Scrape User Agent Strings from useragentstring.com","text":"user agent data automatically updated every Sunday midnight UTC via GitHub Actions. workflow: Scrapes major browsers (Chrome, Firefox, Safari, Opera, Edge, etc.) Scrapes major crawlers (Googlebot, Bingbot, YandexBot, etc.) Rebuilds package website updated EDA Commits pushes changes can also manually trigger update Actions tab.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"pre-scraped-data","dir":"","previous_headings":"","what":"Pre-scraped Data","title":"Scrape User Agent Strings from useragentstring.com","text":"package includes pre-scraped datasets data/ folder: - chrome.rds, firefox.rds, safari.rds, opera.rds, edge.rds - googlebot.rds, bingbot.rds, yandexbot.rds, etc. Combined CSV available user_agents.csv.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Scrape User Agent Strings from useragentstring.com","text":"MIT","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/blocked_content_patterns.html","id":null,"dir":"Reference","previous_headings":"","what":"Blocked content patterns — blocked_content_patterns","title":"Blocked content patterns — blocked_content_patterns","text":"Blocked content patterns","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/blocked_content_patterns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Blocked content patterns — blocked_content_patterns","text":"","code":"blocked_content_patterns()"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/blocked_status_codes.html","id":null,"dir":"Reference","previous_headings":"","what":"Blocked status codes — blocked_status_codes","title":"Blocked status codes — blocked_status_codes","text":"Blocked status codes","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/blocked_status_codes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Blocked status codes — blocked_status_codes","text":"","code":"blocked_status_codes()"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/calculate_blocking_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate blocking score from test results — calculate_blocking_score","title":"Calculate blocking score from test results — calculate_blocking_score","text":"Computes overall blocking score (0-100) individual site test results. Higher scores indicate better success rates (less blocking).","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/calculate_blocking_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate blocking score from test results — calculate_blocking_score","text":"","code":"calculate_blocking_score(test_results)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/calculate_blocking_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate blocking score from test results — calculate_blocking_score","text":"test_results tibble test_ua_all_sites() 'score' column","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/calculate_blocking_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate blocking score from test results — calculate_blocking_score","text":"Numeric blocking score 0 (always blocked) 100 (never blocked)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/calculate_blocking_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate blocking score from test results — calculate_blocking_score","text":"","code":"if (FALSE) { # \\dontrun{ results <- test_ua_all_sites(\"Mozilla/5.0...\") score <- calculate_blocking_score(results) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/default_test_sites.html","id":null,"dir":"Reference","previous_headings":"","what":"Default test sites for blocking analysis — default_test_sites","title":"Default test sites for blocking analysis — default_test_sites","text":"Returns character vector URLs used testing user agent blocking rates. Sites ordered expected strictness level.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/default_test_sites.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default test sites for blocking analysis — default_test_sites","text":"","code":"default_test_sites()"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/default_test_sites.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Default test sites for blocking analysis — default_test_sites","text":"Character vector test URLs","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/default_test_sites.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Default test sites for blocking analysis — default_test_sites","text":"","code":"sites <- default_test_sites() print(sites) #> [1] \"https://httpbin.org/user-agent\" \"https://www.google.com/\"        #> [3] \"https://www.bing.com/\"          \"https://www.wikipedia.org/\"     #> [5] \"https://www.amazon.com/\"        \"https://www.reddit.com/\"        #> [7] \"https://www.linkedin.com/\"      \"https://www.cloudflare.com/\"    #> [9] \"https://www.instagram.com/\""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/detect_blocking.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect if a response indicates blocking — detect_blocking","title":"Detect if a response indicates blocking — detect_blocking","text":"Analyzes HTTP response characteristics determine request blocked.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/detect_blocking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect if a response indicates blocking — detect_blocking","text":"","code":"detect_blocking(status_code, content, response_time)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/detect_blocking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect if a response indicates blocking — detect_blocking","text":"status_code HTTP status code response content Response body content string response_time Time taken response seconds","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/detect_blocking.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect if a response indicates blocking — detect_blocking","text":"list is_blocked (logical), reason (character), score (numeric 0-1)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/detect_blocking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect if a response indicates blocking — detect_blocking","text":"","code":"result <- detect_blocking(403, \"Access Denied\", 0.5) print(result$is_blocked)  # TRUE #> [1] TRUE"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_all_categories.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all available browser/crawler categories from useragentstring.com — get_all_categories","title":"Get all available browser/crawler categories from useragentstring.com — get_all_categories","text":"Get available browser/crawler categories useragentstring.com","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_all_categories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all available browser/crawler categories from useragentstring.com — get_all_categories","text":"","code":"get_all_categories()"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_all_categories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all available browser/crawler categories from useragentstring.com — get_all_categories","text":"tibble columns: name (browser/crawler name), category (CRAWLERS, BROWSERS, etc.)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_all_categories.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get all available browser/crawler categories from useragentstring.com — get_all_categories","text":"","code":"if (FALSE) { # \\dontrun{ categories <- get_all_categories() head(categories) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_browser_links.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all user agent detail links for a specific browser — get_browser_links","title":"Get all user agent detail links for a specific browser — get_browser_links","text":"Get user agent detail links specific browser","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_browser_links.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all user agent detail links for a specific browser — get_browser_links","text":"","code":"get_browser_links(browser)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_browser_links.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all user agent detail links for a specific browser — get_browser_links","text":"browser Character string specifying browser name (e.g., \"Chrome\", \"Safari\", \"Opera\")","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_browser_links.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all user agent detail links for a specific browser — get_browser_links","text":"Character vector detail page URLs","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_browser_links.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get all user agent detail links for a specific browser — get_browser_links","text":"","code":"if (FALSE) { # \\dontrun{ links <- get_browser_links(\"Chrome\") head(links) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_useragent_details.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape details from a single user agent page — get_useragent_details","title":"Scrape details from a single user agent page — get_useragent_details","text":"Scrape details single user agent page","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_useragent_details.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape details from a single user agent page — get_useragent_details","text":"","code":"get_useragent_details(detail_url)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_useragent_details.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape details from a single user agent page — get_useragent_details","text":"detail_url URL path detail page (e.g., \"/Chrome134.0_id_20018.php\")","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_useragent_details.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape details from a single user agent page — get_useragent_details","text":"tibble useragent_string, first_visit, last_visit, detail_url","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/get_useragent_details.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape details from a single user agent page — get_useragent_details","text":"","code":"if (FALSE) { # \\dontrun{ details <- get_useragent_details(\"/Chrome134.0.6998.166_id_20018.php\") } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/run_blocking_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Run comprehensive blocking test on user agents — run_blocking_test","title":"Run comprehensive blocking test on user agents — run_blocking_test","text":"Tests sample user agents multiple websites determine user agents effective web scraping.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/run_blocking_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run comprehensive blocking test on user agents — run_blocking_test","text":"","code":"run_blocking_test(   user_agents,   sites = default_test_sites(),   delay = 0.5,   progress = TRUE )"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/run_blocking_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run comprehensive blocking test on user agents — run_blocking_test","text":"user_agents tibble user agent data (must 'useragent_string' column) sites Character vector URLs test delay Delay requests seconds (default 0.5) progress Show progress bar (default TRUE)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/run_blocking_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run comprehensive blocking test on user agents — run_blocking_test","text":"tibble user agent info blocking scores","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/run_blocking_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run comprehensive blocking test on user agents — run_blocking_test","text":"","code":"if (FALSE) { # \\dontrun{ # Load user agent data ua_data <- readRDS(\"data/chrome.rds\") sample_ua <- ua_data[1:10, ]  # Run blocking test results <- run_blocking_test(sample_ua, sites = default_test_sites()[1:3]) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/scrape_browser.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape all user agents for a specific browser — scrape_browser","title":"Scrape all user agents for a specific browser — scrape_browser","text":"Scrape user agents specific browser","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/scrape_browser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape all user agents for a specific browser — scrape_browser","text":"","code":"scrape_browser(browser, parallel = TRUE, workers = 4, limit = NULL)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/scrape_browser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape all user agents for a specific browser — scrape_browser","text":"browser Character string specifying browser name (e.g., \"Chrome\", \"Safari\", \"Opera\") parallel Logical, whether use parallel processing (default TRUE) workers Number parallel workers (default 4) limit Optional limit number user agents scrape (useful testing)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/scrape_browser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape all user agents for a specific browser — scrape_browser","text":"tibble columns: browser, version, useragent_string, first_visit, last_visit, detail_url","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/scrape_browser.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape all user agents for a specific browser — scrape_browser","text":"","code":"if (FALSE) { # \\dontrun{ # Scrape first 10 Chrome user agents chrome_sample <- scrape_browser(\"Chrome\", limit = 10)  # Scrape all Chrome user agents with parallelization chrome_all <- scrape_browser(\"Chrome\", parallel = TRUE, workers = 4) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_ua_all_sites.html","id":null,"dir":"Reference","previous_headings":"","what":"Test a user agent against multiple sites — test_ua_all_sites","title":"Test a user agent against multiple sites — test_ua_all_sites","text":"Tests single user agent string list websites returns aggregated results including blocking score.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_ua_all_sites.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test a user agent against multiple sites — test_ua_all_sites","text":"","code":"test_ua_all_sites(user_agent, sites = default_test_sites(), delay = 0.5)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_ua_all_sites.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test a user agent against multiple sites — test_ua_all_sites","text":"user_agent User agent string test sites Character vector URLs test (default: default_test_sites()) delay Delay requests seconds (default 0.5)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_ua_all_sites.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test a user agent against multiple sites — test_ua_all_sites","text":"tibble per-site results overall blocking score","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_ua_all_sites.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test a user agent against multiple sites — test_ua_all_sites","text":"","code":"if (FALSE) { # \\dontrun{ results <- test_ua_all_sites(   \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0\",   sites = c(\"https://httpbin.org/user-agent\", \"https://www.google.com/\") ) } # }"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_user_agent.html","id":null,"dir":"Reference","previous_headings":"","what":"Test a single user agent against a URL — test_user_agent","title":"Test a single user agent against a URL — test_user_agent","text":"Makes HTTP request specified URL using given user agent string analyzes response blocking indicators.","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_user_agent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test a single user agent against a URL — test_user_agent","text":"","code":"test_user_agent(url, user_agent, timeout = 15)"},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_user_agent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test a single user agent against a URL — test_user_agent","text":"url Target URL test user_agent User agent string use request timeout Request timeout seconds (default 15)","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_user_agent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test a single user agent against a URL — test_user_agent","text":"tibble columns: url, user_agent, status_code, response_time, is_blocked, reason, score","code":""},{"path":"https://benjaminguinaudeau.github.io/useragentstring/reference/test_user_agent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test a single user agent against a URL — test_user_agent","text":"","code":"if (FALSE) { # \\dontrun{ result <- test_user_agent(   \"https://httpbin.org/user-agent\",   \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0\" ) } # }"}]
